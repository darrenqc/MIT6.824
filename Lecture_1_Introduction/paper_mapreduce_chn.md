# MapReduce: 运行在大集群上的简化的数据处理流程

### 摘要

MapReduce是一个应用于大规模数据集处理和生成的编程模型和对应的编程实现。用户需要定义map函数和reduce函数，map函数处理输入键值对，生成中间数据键值对，reduce函数合并具有相同键的中间数据。如同本论文中展示的，许多现实世界中的任务都可以用这个模型描述。

MapReduce的代码符合函数式编程范式，并行地运行在一个大规模的商用机器集群里。MapReduce运行时负责输入数据分片、调度程序执行、处理机器失败、管理机器间通信。这让一个没有分布式和并行计算的程序员可以简易地使用大规模分布式系统的资源。

我们的MapReduce实现运行在一个大规模的商用集群里，具有很高的扩展性：典型的MapReduce计算运行在数以千计的机器上，处理着GB级别的数据。程序员们认为MapReduce系统简单易用：目前已开发完成了数百个MapReduce程序，每天有近一千MapReduce任务在谷歌集群里运行。

### 1 简介

在过去5年间，作者们及很多其他谷歌的程序员写了数百个计算程序来生成不同的衍生数据，比如倒排索引、网页的图结构、每个主机被爬取到的页面数量、某天访问频次最高的查询等等。这些计算程序需要处理大量原始数据，比如爬取的文档、网页访问日志等等。这些计算大多目的很明确。然而，输入数据集往往很庞大，为了在可接受的时间内完成计算，计算任务必须分配到成百上千台机器执行。如何并行计算、分发数据、处理异常让原本简单直接的计算变得异常复杂。

为了应对分布式计算的复杂性，我们把并行计算、容错、数据分发、负载均衡都封装到一个代码库里，调用方只需要关心计算逻辑。我们的抽象设计受到了Lisp和其他一些函数式编程语言的里面内置的map函数和reduce函数的启发。我们发现，大部分的计算过程中都涉及到将原始数据映射为一个个键值对中间数据，再将这些中间数据按照相同键聚合。运用函数式编程模型让并行计算变得简单，让容错可以通过重试解决。

这个项目的主要贡献是提供了简单强大的可以实现自动化并行计算的接口，并且提供了一个可以运行在大规模商用机器集群上的高性能实现。

第2部分描述了该编程模型的基本原理，提供了一些简易的例子。第3部分描述了为了基于集群的计算环境设计的MapReduce接口以及实现。第4部分描述了我们针对这个编程模型的一些优化。第5部分给出了计算不同任务的性能统计。第6部分展示了MapReduce在谷歌内部的应用，包括如何以此为基础重写我们生产环境的索引系统。第7部分讨论了相关工作和未来工作。

### 2 编程模型

这个计算模型的输入是键值对，输出也是键值对。使用者用Map和Reduce这两个函数来描述计算过程。

Map函数，由使用者给出的，读取输入键值对，生成中间键值对。MapReduce把所有中间数据按键分组，传递给Reduce函数。

Reduce函数，也由使用者给出，接收中间键值对分组而成的键，以及该键对应的所有值，然后把这些值合并，得到一个更小的数据集。一般来说，一次Reduce函数调用只会产生一个或零个输出值。中间键值对通过一个迭代器传递给Reduce函数。这让我们可以处理无法装载进内存的大量键值对。

#### 2.1 例子

以下面这个场景为例，统计每个单词在众多文档中出现的频率，用户会写类似下面的伪代码：

  ```
    map(String key, String value):
      // key: document name
      // value: document contents
      for each word w in value:
        EmitIntermediate(w, "1");

    reduce(String key, Iterator values):
      // key: a word
      // values: a list of counts
      int result = 0;
      for each v in values:
        result += ParseInt(v);
      Emit(AsString(result));
  ```

Map函数产出每个单词以及对应的出现次数(出于简单上面伪代码里设置为'1')。Reduce函数将某个特定词的所有出现次数加总。

除此以外，用户需要按照MapReduce规范，用mapreduce specification object来封装输入输出文件以及一些调优参数。然后把这一object传递给MapReduce函数，并调用MapReduce函数。用户代码与MapReduce代码(用C++实现)相互关联。附录A有这个例子的完整代码。

#### 2.2 类型

前面的伪代码以字符串作为输入和输出，理论上用户提供的map函数和reduce函数应该具有相关联的数据类型：

  ```
    map     (k1,v1)        ->  list(k2,v2)
    reduce  (k2,list(v2))  ->  list(v2)
  ```

输入键值对和输出键值对来自不同类型的数据。中间键值对和输出键值对具有同样的类型。

我们的C++实现把字符串传递给用户定义的map和reduce函数，用户需要自己处理好数据类型的转换。

#### 2.3 更多例子

以下是一些简单有趣的可以用MapReduce来描述的计算场景。

分布式的grep函数：map函数输出所有匹配给定模式的行，reduce函数直接输出所有收到的数据即可。

统计URL访问频次：map函数处理页面访问日志，输出<URL,1>，reduce函数将相同URL的频次相加，输出<URL,total count>。

反向网页连接图：map函数读取source页面每一个链接target，输出<target,source>，reduce函数将相同target的键值对的值拼接起来，输出<target,list(source)>。

各个主机的词向量：词向量总结了一个文档或者多个文档中最重要的词，以<word,frequency>的形式展示。map函数读取每一个文档，从文档的URL中提取hostname，输出<hostname,term vector>，reduce函数接收各个host的所有term vectors，将这些term vectors拼接在一起，去掉低频的词，输出<hostname,term vector>

倒排索引：map函数解析各个文档，输出<word,document ID>，reduce函数接收每个词对应的键值对，将document ID排好序，输出<word,list(document ID)>。输出结果集就是一个简单的倒排索引，通过追踪词语的位置来优化这个倒排索引是很简单的。

分布式排序：map函数提取每条记录的key，输出<key,record>，reduce函数直接输出收到的数据。这个计算需要依赖分片功能(Section 4.1)和分片功能的有序性质(Section 4.2)。

### 3 实现

MapReduce接口有很多可行的实现，需要根据环境来选择最佳的实现方式。一种实现方式比较适合一台小的共享内存的机器，一种实现方式适合大型NUMA架构多处理器集群，另一种适合更大型的网络互联的集群。

这一部分聚焦在谷歌的计算环境，一个由大量商用计算机通过以太网交换机互联的集群。在我们的环境里：

(1) 机器大多是双核x86处理器，2-4GB内存，linux系统；

(2) 使用了商用网络硬件，大多带宽为100M bit/s，机器层面带宽为1G bit/s，但平均来说，总体对分带宽会更小；

(3) 一个集群由成百上千个集群组成，所以失败很常见；

(4) 存储使用的是不贵的IDE硬盘，直接挂载在每一台机器上。谷歌内部开发了一个分布式文件系统来管理这些机器上的数据。这个分布式文件系统通过冗余来保证在不可靠的机器上实现可用性和可靠性；

(5) 用户做作业提交给调度系统。每个作业包含了一系列任务，并由调度系统分配给集群里可用的机器。

#### 3.1 程序执行总览

map分布式地运行在多台机器上。MapReduce将输入数据分成M份，每一份都可以在不同的机器上被并行地处理。reduce也分布式地运行在多台机器上，partitioning通过将中间键值对按照key的hash值分为R份(比如hash(key) mod R)，这个分片的数量R由用户指定。

图 1 展示了我们MapReduce实现的整理流程。当用户程序调用MapReduce时，以下这些动作将按顺序发生(图 1 中标注的数字与下面的列表相对应)。

1. MapReduce将输入文件分成M份，每份大小通常是16-64MB(用户可以通过参数设置)，然后会启动集群机器上MapReduce的副本；

2. 其中一个MapReduce副本比较特殊 - master。其他worker都由master分配任务。一共有M个map任务和R个reduce任务。master挑选一个空闲的worker，并指派一个map任务或者一个reduce任务；

3. 被指派了map任务的worker会读取相应的输入分片，解析输入数据里的键值对，传递给用户定义的map函数，生成的中间键值对缓存在内存里；

4. 缓存的中间键值对会周期性地被写入本地磁盘，并由partioning分成R份，这些中间键值对的存储位置被传递给master，master负责将这些位置告诉reduce worker；

5. 当reduce worker被master通知到中间键值对的位置，它会通过RPC读取这些数据。读取完毕后，它会根据中间键值对的key来排序这些数据，让具有相同key的键值对排在一起。排序是有必要的，因为一个reduce任务里需要处理不同的key。假如数据量大到无法装载进内存，需要使用外部力量来进行排序；

6. reduce worker逐一遍历这些排好序的中间数据，把每一组中间键值对传递给用户定义的reduce函数，生成的结果会被输出到这个reduce分片对应的结果文件里；

7. 当所有map任务和reduce任务都完成了，master会唤醒用户程序。此时，用户程序里的MapReduce函数调用结束，将程序控制权交回用户代码手里。

当全过程成功结束时，MapReduce执行结果为R个输出文件(每个reduce任务一个文件，文件名由用户定义)。一般来说，用户不需要合并这R个输出文件，这R个输出文件会作为下一个MapReduce输入，或者被另一个可以管理分片文件的分布式系统使用。

#### 3.2 Master节点的数据结构

master需要保存多个数据结构。对于每一个map任务和reduce任务，master需要记录任务的状态(空闲、运行中、完成)以及分配的worker的身份标识。

master需要将map任务产生的中间结果的位置告诉reduce任务，所以，对于每个完成的map任务，master需要保存生成的中间结果的位置和大小。当map任务完成时，master会更新中间结果的位置和大小信息。这一信息会增量地推送给正在运行reduce任务的worker。

#### 3.3 容错性

MapReduce是被设计来给大规模集群处理大量数据的，必须处理好容错。

##### Worker失败

master会周期性地ping每一个worker，如果一段时间内没有收到某个worker的响应，master会把该worker标记为失败。所有由这一机器完成的map任务都会被标记为它们初始的空闲状态，等待被重新分配给其他worker。类似地，正在该worker上运行的map任务或者reduce任务都会被重置为初始的空闲状态，等待再次分配。

当worker失败时，已完成的map任务也需要重新执行是因为中间结果保存在失败worker的本地磁盘，无法读取。已完成的reduce任务不需要重新执行，因为结果已经被保存在全局文件系统(Global File System or GFS)里。

假如一个map任务一开始由worker A执行，后来转交worker B执行(因为A失败了)，所有正在执行reduce任务的worker都会被通知到，还没从worker A读取数据的reduce worker会转而从worker B读取数据。

MapReduce是可以从大规模worker失败中恢复的。比如，在某次MapReduce作业中，网络维护让集群里80台机器变得在几分钟里不可用，master将所有由该80台机器执行的任务重新分配给其他机器执行一遍，依然能取得正向进展，最终完成作业。

##### Master失败

让master周期性地把上面提及的master需要存储的数据结构持久化是很容易的。如果master宕机了，可以从上一个存储点数据启动一个新的master。但其实因为只有一个master节点，宕机的可能性是很小的，所以我们目前实现的MapReduce在master节点宕机时会放弃当前任务。如果有必要的话，用户可以重启任务。

##### 当失败存在时的语义

如果用户提供的map函数和reduce函数的输出仅仅由它们的输入参数决定，那么我们的分布式系统产生的结果一定与没有出错顺序完成整个计算的情况下产生的结果一致。

我们产生结果的一致性依赖于map和reduce任务产生结果的原子性。每个正在进行的任务都会把它的结果输出到私有的临时文件。一个reduce任务会产生一个这样的文件，而一个map任务会产生R个这样的文件(每个reduce任务都需要其中的一个)。当一个map任务完成时，worker会把R个文件的位置告诉master。如果master收到一个已被标记完成的map任务的信息，master会选择忽略这条信息，否则，master会把这R个文件的位置保存在master的数据结构中。

当reduce任务完成时，reduce worker会把临时输出文件重命名为最终输出文件，这个重命名过程是原子的。如果同一个reduce任务被多个worker完成了，那么重命名会多次发生，但最终输出依然是一致的。仰仗底层的文件系统的重命名具有原子性，我们的系统最终只会存在一份reduce任务的输出。

我们的map函数和reduce函数的输出结果仅由输入参数决定，我们的操作与顺序执行具有相同的语义，因此，用户可以很好地把握他们的程序的行为。如果map函数或者reduce函数是非确定性的，我们依然能够提供一个稍弱但可接受的语义。如果存在非确定性的操作符，某个特定的reduce任务R1的输出与某一个执行顺序下产生的R1一致，但另一个reduce任务R2很可能与另一种执行顺序下产生的R2一致。

我们来分析map任务M和reduce任务R1及R2。令e(Ri)表示Ri的执行。当e(R1)读取的M的输出与e(R2)读取的M的输出不一样时，弱语义就产生了。

#### 3.4 本地性

在我们的计算环境里，网络带宽是很稀缺的资源。GFS管理的数据实际上存储在本地，我们利用这一点来节约网络带宽。GFS把每个文件分成64MB的文件块，每个文件块都会被冗余多份(通常3份)保存在不同机器上。MapReduce的master会考虑到输入文件的实际存储地址，尝试着把map任务分发给本地存有输入文件副本的worker。如果失败了，master会尝试着把任务分配给离输入文件更近的worker(比如与输入文件存储机器使用同一交换机节点的worker)。当在大部分节点上运行大规模MapReduce计算时，大部分的数据都是通过本地文件输入，几乎不占用网络带宽

#### 3.5 任务颗粒度

如上所说，我们把map任务细分为M份，把reduce任务细分为R份。理想状态下，M和R应该远大于worker的数量。让每个worker执行大量不同的任务有助于动态负载均衡，也会让系统从部分worker的失败中恢复得更快：失败worker完成的map任务可以重新分配给其他worker。

在我们的MapReduce实现中，M和R的大小是应该要有上限的，因为master需要花费O(M + R)的时间复杂度来分发任务，需要花费O(M * R)的空间复杂度来存储状态。(内存的固定开销很小，在O(M * R)空间复杂度的内存开销里，每个map或者reduce任务的固定开销约为1字节)。

而且，一般来说R的大小都是用户指定的，因为每个reduce任务都会产生一个单独的输出文件。在实际应用中，我们通常会选择一个适当的M值，让每个map任务的输入大小在16-64MB(为了最大化本地优化的效果)，让R是worker数量的一个较小的倍数。我们经常设置M=200000，R=5000，在2000个worker的集群里运行MapReduce。

#### 3.6 备份任务

MapReduce运行时长过长一个很常见的原因是短板：其中一台机器最后几个map或者reduce任务花费了过长时间。很多原因都会导致短板的发生。比如，如果一台机器的硬盘坏了，会导致系统频繁地修复硬盘错误，最终导致读取速度从30MB/s降低到1MB/s。又比如集群可能给一台机器分配了别的任务，导致与MapReduce任务产生对CPU、内存、磁盘、带宽的竞争。最近我们碰到的一个问题是机器初始化代码有bug，导致处理器缓存被禁用了，受这个bug影响的机器速度下降了超过99%。

我们有一个通用的机制去减轻短板带来的影响。当MapReduce接近尾声时，master会把剩下的任务复制一份分发出去。如此一来，原任务或者任务副本完成，该任务就完成了。我们调整了这个机制，以使得计算量只增加几个百分点。我们发现大大节省了大型MapReduce任务的总耗时。作为一个例子，如果取消这个机制，5.3的排序任务会花费额外44%的时间。

### 4. 调优

虽然基本的map函数和reduce函数已经能满足大部分需求了，我们发现一些扩展功能还是有用武之地。我们会在这个部分介绍这些扩展功能。

#### 4.1 Partition函数

MapReduce的用户会根据自己的需求选择reduce任务的个数R。中间数据会基于partition函数被分片供reduce任务使用。默认的partition函数使用hash来分片(hash(key) mod R)，在大部分情况下，分片都是平衡的。但是在某些场景下，用别的partition函数会更合适。比如，有时候输出的key是URL，我们希望所有host一样的URL都被分配到同一个文件里。为了支持这样的场景，MapReduce允许用户提供自己的partition函数。我们可以选择(hash(Hostname(URL)) mod R)左右partition函数，这样host一样的URL就会被分配到同一个文件里。

#### 4.2 有序机制

我们保证在一个分片里，中间键值对会按key升序被处理。这种有序机制让输出有序的reduce结果变得容易，有序的结果意味着高效的查找，用户会发现结果有序会带来很多方便。

#### 4.3 Combiner函数

在某些情况下，map任务会产生很多具有相同key的中间键值对。2.1的统计词频是个很好的例子，词频往往符合Zipf分布，每个map任务都会生成成百上千个<the, 1>数据。这所有相同的数据都会通过网络发送给一个reduce任务，然后被reduce任务加总，输出一个数字。我们允许用户定义一个可选的combiner函数，在通过网络发送数据之前先合并一次数据。

每个map worker都会执行combiner函数。通常reduce函数和combiner函数会使用同样的代码，唯一的区别是MapReduce处理这两个函数输出结果的方式不同。reduce函数的输出会被写到最终的输出文件里，而combiner函数的输出会被写到中间文件里，供reduce函数使用。

部分合并在某些情况下大大提高了MapReduce的效率。附录A里提供了一个使用combiner的例子。

#### 4.4 输入和输出类型

MapReduce库支持读取几种不同类型的输入。"text"模式认为输入的每一行都是键值对：key对应行号，value对应该行的内容。另一种常见的格式是按key排好序的键值对。每一种输入类型的实现都会处理如何将输入文件分片为多个map函数的输入("text"模式会保证分片里同一行不会被切开)。用户可以通过实现reader接口来添加一种新的输入模式，虽然大部分用户其实都只使用了MapReduce库预定义的输入模式少数几个。

reader不一定要从文件读取数据。定义一个从数据库读取数据或是从内存读取结构化数据的reader是很简单的。

类似地，我们支持一系列不同格式的输出，用户也可以根据自己的需要实现对新的输出格式的支持。

#### 4.5 副作用

在某些情况下，MapReduce会产生额外的文件(map任务或reduce任务的副本产生的结果)。我们是通过GFS重命名文件的原子性来保证结果的原子性和幂等性。

我们不提供多文件的两阶段提交，假如一个任务产生多个文件，我们无法保证过程的原子性。因此，如果任务要生成多个结果文件，并且需要多个文件保持一致性，任务本身必须具备确定性。实际应用中，这个限制并不是一个很大的问题。

#### 4.6 忽略脏数据

有时候用户代码里有bug，特定的输入必然会导致map函数和reduce函数发生异常。这些bug会导致MapReduce作业无法完成。通常解决方案是修复bug，但有时这不可行，也许一个源码不可见的第三方库有bug。而且，有时候忽略小量的数据是可接受的，比如统计一个庞大的数据集时。我们还提供另一种执行模式，这种执行模式会自动判断并忽略必然会引起异常的输入，以保证作业取得进展。

每个worker都会安装一个信号处理者，用户捕获分片冲突和信息通路错误。在执行用户定义的map函数和reduce函数之前，MapReduce库会把输入的序号保存在全局变量里。如果用户代码产生了信号，信号处理者会发一个携带输入序号的UDP消息给MapReduce master，当master发现某一条特定的数据产生了超过一次失败，在下次执行对应的map或者reduce任务时，这条数据会被忽略掉。

#### 4.7 本地执行

实际的计算过程发生在分布式系统中，通常涉及数以千计的节点，任务分配由master节点动态完成，因此调试map函数和reduce函数里的问题是很困难的。为了让断点调试、性能分析、小规模测试更加简便，我们实现了一个本地版的MapReduce，所有的任务都在本地顺序完成，用户能够精准地控制让MapReduce库只执行特定的map任务。用户可以在启动程序的时候增加一遍标志位来使用本地模式，然后本地调试工具就可以派上用场了。

#### 4.8 状态信息

master上还运行着一个HTTP服务器，供用户查询一些列状态。状态页会展示运行的进度，比如已经完成了多少个任务、有多少个任务在进行中、输入的大小、中间数据的大小、输出的大小、处理的速度等等。这些状态页面里还有跳转到每个任务的标准输出和标准错误输出的链接。用户可以根据这些数据去预测计算所需时长，计算是否需要更多资源。这些页面也可以用来发现计算比预期缓慢的情况。

除此以外，主状态页面还展示那些worker失败了，他们失败时在执行哪些map任务或者reduce任务。这些信息对诊断用户代码中的bug很有帮助。

#### 4.9 计数器

MapReduce库提供了一个计数器来统计不同事件的发生次数。比如，用户可能想要统计处理的词的数量，或者建立索引的文档数量等等。

为了使用计数器功能，用户需要定义一个counter对象，并在map函数或者reduce函数里恰当地增加计数器的值。比如：

```
  Counter* uppercase;
  uppercase = GetCounter("uppercase");

  map(String name, String contents):
    for each word w in contents:
      If (isCapitalized(w)):
        uppercase->Increment();
      EmitIntermediate(w, "1");
```

每个worker都有一个计数器，会周期性地传递给master(通过ping的响应传递)。master会聚合成功的map函数或者reduce函数的计数器数值，并在作业完成时将数值返回给用户代码。用户也可以从状态页里看到当前的计数器数值，从而了解计算的进度。在聚合数值时，master会考虑到接收到的map任务或者reduce任务是否已经完成过，不会重复计算(任务副本和失败任务都有可能导致任务呗重复执行)。

MapReduce库会自动维护一些计数器，比如处理的键值对数量、产生的键值对数量。

计数器给用户掌握MapReduce的行为提供了便利。比如，用户可能会希望生成的键值对数量与处理的键值对数量一致，或者德语文档数量占总文档数量的比重在一个可接受的范围内。

### 5. 性能

在这一部分，我们将会测量两个运行在大集群里的MapReduce作业的性能。其中一个任务是在GB量级的数据里寻找满足特定模式的字符串。另一个任务是对约为1GB的数据进行排序。

#### 5.1 集群配置

所有的程序都运行在约为1800台机器的集群里。每一台机器都拥有两个2GHz的英特尔Xeon系列具有超线程技术的处理器，4GB内存，两个160GB的IDE硬盘，100G bit的带宽。这些机器构成一个两层树状结构的网络，通过交换机相连，根节点的带宽约为100-200G bit/s。所有的机器都在同一个局域网内，所以每两台机器的响应时间都小于1ms。

4GB的内存里，大约需要预留1-1.5GB给其他的任务使用。程序在周末下午运行，这时大多数的CPU、磁盘、网络都是空闲的。

#### 5.2 Grep

grep程序读取了100亿条大小为100字节的数据，搜索一个很罕见的由三个字母组成的模式(总共有92,337条记录符合该模式)。输入被分为大约64MB大小的分片(M=15000)，输出到一个文件里(R=1)。

图 2 展示了计算进度随着时间推移的变化。Y轴表示读取输入的速度，可以看到随着更多的机器加入计算，读取速度越来越快，当1,764台机器加入计算时，速度达到的顶峰的30GB/s。当map任务都完成时，读取输入速度开始下降，在80s内降为0。整个计算过程从开始到结束总共耗时150s。这包括了大约1分钟的启动开销。启动开销包括了把程序复制到每一台机器时间，以及为了读取1000个输入文件及其位置与GFS交互的时间。

#### 5.3 Sort

sort程序排序了100亿条大小为100字节的数据(总数据量约为1GB)。这个程序是基于TeraSort设计的。

这个排序程序只有不到50行的用户代码。一个三行的map函数负责从输入文本里提取出10字节的用于排序的key，把原始输入文本作为value，组成中间键值对。我们使用了内置的Identity函数作为Reduce函数。这个函数直接把中间键值对返回作为输出。最终的输出结果以两副本文件写入GFS(最终产生了2GB的输出)。

与上一个程序一样，输入被分为64MB大小的分片(M=15000)，我们把输出结果写入4000个文件里(R=4000)。Partition函数使用key的第一个字节把数据写入对应的R个输出文件。

我们根据预先知道的key的分布设计的Partition函数。通常情况下，我们应该以一部分数据作为样本，先找到key的分布，再确定如何设计Partition函数。

图 3(a) 展示了计算进度随着时间推移的变化。左上角的图展示了读取输入的速度。速度的峰值达到13GB/s，在200s之前，因所有的map任务都完成而降为0。需要注意的是这个速度比上一个grep程序要慢，因为sort的map任务需要花费一般的时间和磁盘IO把中间数据输出到本地磁盘，而grep对应的开销几乎可以忽略不计。

左边中间的图片展示了reduce任务接收map任务结果的速度。这个洗牌从第一个map任务结束就开始了。第一个波峰是第一批约为1700个reduce任务引起的(总机器数为1700，每个机器同时只能执行一个reduce任务)。大约300秒过去，第一批任务完成了一部分，开始为剩下的reduce任务进行中间数据洗牌。在600s时，所有的数据洗牌都完成了。

左下方的图展示了reduce任务排序数据并写入输出文件的速度。写入输出开始略晚于第一批数据洗牌结束是因为机器正在对中间数据进行排序。写入以2-4GB/s的速度进行着，在大约850s的时候完成了整个作业。包括启动任务的开销，整个计算过程耗时891s。这与目前已知最好的TeraSort性能相近。

有一些事情需要注意：输入速度大于洗牌速度和输出速度，这是因为我们的本地优化，大多数数据都是从本地磁盘读入的，并不会通过我们带宽受限的网络。洗牌速度比输出速度快，这是因为输出过程需要写入两份排好序的数据(为了可靠性和可用性，我们存储两份输出结果)。我们保存两个副本是因为这是我们基于的文件系统保证可靠性和可用性的机制。如果基于的文件系统使用纠删码而不是冗余来保证可靠性和可用性，写入数据对网络带宽的要求可以进一步降低。

#### 5.4 备份任务的影响

图 3(b) 展示了禁止备份任务时的情况。程序执行的流程与图 3(a) 类似，除了有一段很长的几乎没有写入发生的长尾阶段。在960s之后，只剩5个reduce任务还未完成，然而这几个拖油瓶直到300秒之后才结束。整个计算过程花费了1283秒，耗时增加了44%。

#### 5.5 机器失败

图 3(c) 展示了在作业开始之后，我们故意杀死其中200台机器的worker进程时候的情况。集群调度者立即在这些机器上重启worker进程(这些机器还是正常运作的，只是进程被杀死了而已)。

worker失败会造成负的输入读取速度，因为之前完成的map任务需要重新执行(因为对应的worker进程被杀死了)。map任务的重新分配相当迅速。整个计算程序在933秒内结束了，包含程序启动的开销(只比正常执行增加了5%的耗时)。