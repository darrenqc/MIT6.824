# 6.824 2018 第一讲: 介绍

## 6.824: 分布式系统开发

### 什么是分布式系统
  * 多电脑协作
  * 大型网站的数据存储，映射归约（MapReduce），点对点的文件共享（peer-to-peer sharing）
  * 很多重要的架构都是分布式的

### 为什么要使用分布式
  * 组织管理物理层面独立的多个个体
  * 通过隔离保证安全性
  * 通过冗余保证容错性
  * 通过并行的CPU计算、内存IO、磁盘IO、网络IO达到线性提升系统处理能力

### 然而:
  * 复杂性: 多个并发的任务
  * 必须处理部分失败
  * 需要通过很巧妙的手段最大化系统性能

### 为什么要参加这门课？
  * 有趣 -- 困难的问题，强大的解决方案
  * 现实中有很多使用场景 -- 大型网站的崛起驱动了分布式系统
  * 活跃的研究领域 -- 获得了很多进展，但还有很多悬而未决的问题
  * 动手实践 -- 你将会在实验课里搭建分布式系统

## 课程结构

### http://pdos.csail.mit.edu/6.824

### 相关教职人员:
  * Malte Schwarzkopf, lecturer
  * Robert Morris, lecturer
  * Deepti Raghavan, TA
  * Edward Park, TA
  * Erik Nguyen, TA
  * Anish Athalye, TA

### 课程资料:
  * 讲义
  * 阅读材料
  * 两个考试
  * 实验课
  * 期末实验课（可选）
  * 助教QA时间
  * 可以获得通告和实验帮助的piazza

#### 讲义:
  * 总体思想，论文讨论，实验课

#### 阅读材料:
  * 研究论文，有经典的，也有最新的
  * 这些论文阐述了关键思想和一些重要细节
  * 很多讲义都以这些论文为中心
  * 请在上课前认真阅读这些论文
  * 我们为每篇论文都准备了一个简短的问题
  * 你必须针对每篇论文提出一个问题并发给我们
  * 在讲义前一晚的凌晨前提交QA问题

#### 考试:
  * 其中考试（随堂考试）
  * 期末考试（最后一周）

#### 实验目标:
  * 加深对某些重要技术的理解
  * 获得分布式编程的经验
  * 第一个实验截止时间是第二个周五
  * 第一个实验后基本每周一个实验

##### 实验1: 映射归约（MapReduce）
##### 实验2: 使用Raft算法实现可容错的复制集
##### 实验3: 可容错的键值存储
##### 实验4: 分片的键值存储
##### 最后一次实验（可选）
  * 分成2或3人的小组进行
  * 可以用来代替实验4
  * 你提出一个想法并与我们一起实现
  * 编码，简短的介绍，在最后一天演示

实验分数取决于你通过的测试用例数量，我们给你提供测试用例来判断你的实验是否做的足够好。注意：假如大部分的提交都成功，偶尔会失败，那么大概率当你线上运行的时候会失败

##### Debug会花费很多时间
  * 尽早开始
  * 在助教QA时间寻求帮助
  * 在Piazza上提问

## 主题

### 这是一门关于架构的课程
  * 是对具体应用不可见的分布式系统的抽象层
  * 有三大类抽象：
    - 存储
    - 通信
    - 计算
  * [ 示意图：用户，应用服务器，存储服务器 ]

### 以下几个主题会反复出现

### 主题: 实现（Implementation）
  * 远程调用（RPC）, 线程（threads）, 并发控制（concurrency control）

### 主题: 性能（Performance）
  * 理想状态：可伸缩的吞吐量
    - N个服务器 -> N倍总吞吐量，N台机器并行CPU计算、磁盘IO、网络IO
    - 处理更大的负载仅需要购买更多的机器
  * 伸缩的难度会随着N增大变得越来越难:
    - 负载分配容易不平衡
    - 处理系统里不可并发的部分也变得困难，例如初始化、交互
    - 系统共享资源的瓶颈，例如网络
  * 并不是所有性能问题都可以通过分布式解决
    - 例如，减少单个用户请求的响应时间更多地需要优化代码而不是靠堆砌更多的机器

### 主题: 容错（fault tolerance）
  * 数以千计的服务器组成的复杂的网络意味着总有节点会发生异常
  * 我们不希望应用方感知到这些异常
  * 通常我们希望:
    - 可用性 -- 无论失败是否发生应用都能取得进展
    - 持久性 -- 当失败被修复时应用能恢复工作
  * 大体思路：复制集
    - 如果一个服务器宕机，应用还可以切换到其他复制集继续工作

### 主题: 一致性
  * 通用性架构的行为定义应该合理
    - 例如，“Get(k)应该返回最近一次Put(k,v)的值”
  * 实现合理的行为并不容易！
    - 很难确保每个复制集都一模一样
    - 应用方很可能在多步更新操作中的任何一步失败
    - 服务器可能在很尴尬的时候宕机，例如，执行完成但还没返回结果的时候
    - 网络不畅可能会让你误以为正常运作的机器宕机了，“脑分裂”的风险
  * 一致性和性能是互相矛盾的
    - 一致性需要通过通信来实现，例如，获得最近一次Put()的结果
    - 强一致性通常意味着系统响应很迟钝
    - 高性能通常意味着弱一致性
  * 人们在这个领域已经总结出很多设计要点

## 案例分析: MapReduce

### 让我们以MapReduce(MR)为学习案例，MR是6.824的一个很好的阐述，也是实验1的重点

### MapReduce概况
  * 场景：涉及数以GB计的数据耗时数以小时计的计算任务
    - 例如，分析爬虫获得的网页的图结构
    - 只适用于1000+服务器节点
    - 通常不是分布式系统专家设计的
    - 分布式的开发室极其痛苦的，例如，处理失败
  * 总体目标：让非专家级的程序员也可以轻易地将大量数据分发到多台服务器高效地处理
  * 程序员定义Map和Reduce函数，顺序的代码，一般比较简单
  * MR将函数运行在1000+机器节点，处理大量输入并隐藏分布式的细节

### MapReduce的抽象视图
  * 输入被分为M个文件
  * [ 示意图: maps函数生成键值对形式的行数据， reduces函数消费列数据 ]

    ```
      Input1 -> Map -> a,1 b,1 c,1
      Input2 -> Map ->     b,1
      Input3 -> Map -> a,1     c,1
                        |   |   |
                        |   |   -> Reduce -> c,2
                        |   -----> Reduce -> b,2
                        ---------> Reduce -> a,2
    ```

  * MR为每个输入文件调用Map()函数，生成中间数据集<k2,v2>，每次Map()函数调用都是一个“任务”
  * MR收集中间数据集里某个给定k2对应的所有v2，并把他们作为参数传递给Reduce函数
  * 最后的输出是Reduce()函数产生的数据集<k3,v3>，保存在R个输出文件里
  * [ 示意图: MapReduce API ]

    ```    
      map(k1, v1) -> list(k2, v2)
      reduce(k2, list(v2)) -> list(k2, v3)
    ```

### 例子: 词频统计
  * 输入是成千上万个文本文件

    ```
      Map(k, v)
        split v into words
        for each word w
          emit(w, "1")
      Reduce(k, v)
        emit(len(v))
    ```

### MapReduce隐藏了繁琐的细节:
  * 在各个服务器上启动程序
  * 追踪各个任务的完成进度tracking which tasks are done
  * 数据流动
  * 从失败中恢复

### MapReduce拥有很好的扩展性:
  * N台服务器可以提供N倍的吞吐量。
    - 假设M，R >= N (考虑很多输入文件，很多Map函数的输出键的场景)，Map()函数可以并行运行，因为他们不需要交互。Reduce()函数也一样。
    - 唯一的交互是让数据在Map()函数和Reduce()函数间随机洗牌。
  * 所以你可以通过添加服务器来获取更大的吞吐量。
    - 相比针对每个应用场景优化效率的秉性方法，服务器要廉价得多。

### 可能限制性能的因素有哪些？
  * 我们在乎这些因素，因为这些因素正是需要优化的地方。
  * CPU? 内存? 磁盘? 网络?
  * 2004年，“网络带宽”成为了MapReduce的作者们提升性能的瓶颈
    - [ 示意图: 服务器集群, 网络交换机树状图 ]

      ```
        Map->Reduce随机洗牌过中所有的数据都通过网络传输
        论文里交换机根节点：100 到 200 G bit/s
        1800个服务器节点，所以每台服务器分到的带宽是 55 M bit/s
        这个带宽太小了，远远小于当时的磁盘IO (~50-100 MB/s) 和 RAM 的速度
      ```
  
  * 于是他们关注的是最小化数据在网络内的流动(今时今日数据中心的网络速度已经远远超过当时的水平)

### 更多细节 (论文中 图 1):
  * master节点：分配任务给worker节点，记住中产出的位置
  * M个Map任务，R个Reduce任务
  * 输入存储在GFS(Global File System)，每个输入文件都有3个副本
  * 所有服务器上都运行着GFS和MR workers
  * 输入任务远大于worker数量
  * master给每个worker分配一个Map任务，当该Map任务完成再分配一个新的Map任务
  * Map worker将产生的keys根据哈希值分成R个部分,存储在本地磁盘
  * 问题: 针对这个场景应该如何设计数据结构？
  * 在所有的Map任务都完成前不执行Reduce任务
  * master告诉Reduce worker获取Map worker生成的中间数据
  * Reduce workers将最终结果输出到GFS (每个Reduce任务一个输出文件)

### 有哪些针对网络带宽不足而优化的设计？
  * Map的输入是从本地GFS副本中读取的，而不是通过网络
  * 中间数据仅在网络中传输了一次
    - Map worker将中间结果输出到本地磁盘，而不是GFS中
  * 中间数据被分到对应很多key的文件中
    - 问题: 为什么不将Map函数执行过程中的输出实时通过TCP传输到Reduce worker处理？

### 如何做到负载均衡？
  * 对扩展性来说至关重要 -- 让N-1个节点等待1个节点是很低效的。
  * 但是某些任务很可能比别的任务耗时要长
  * [ 示意图: 将长度不确定的任务分配给各个节点 ]
  * 解决方案: 让任务数量远大于节点数量
    - Master将新任务分配给完成任务的worker
    - 于是没有单一任务能主宰完成时间
    - 于是处理任务快的节点会比慢的节点处理更多的任务，并且跟慢的节点几乎同时结束

### 那么容错呢？
  * 比如说其中一台服务器在MR过程中宕机了呢？
  * 使失败不对外可见是保证框架易用性很重要的一部分工作！
  * 问题：为什么不从头开始整个作业呢？
  * MR仅仅会重新运行失败的Map()和Reduce()任务
    - MR要求Map()和Reduce()是纯函数，这意味着：
      * 他们在被调用时不记录状态
      * 他们不会读取或写入除MR输入和输出以外的文件
      * 任务之间没有相互通信
    - 以此来保证重新执行会产出一样的结果
  * 相比其他并行计算方案，MR要求Map()和Reduce()是纯函数，这是MR主要的局限所在，但这对于保证MR的简易性至关重要

### 详细介绍worker如何从宕机中恢复：
  * Map worker宕机:
    - master发现worker不在响应ping请求
    - 已宕机的worker的Map()任务输出丢失了，后续的Reduce()任务很可能会依赖于这一中间数据
    - master基于输入在GFS里的其他副本重新分发任务
    - 部分Reduce worker可能已经读入已宕机worker的中间数据，我们靠Map()函数的纯函数性质来保证两次运行结果一致
    - 如果Reduce worker已经读入所有中间数据，master不需要重新运行Map()任务，但假如Reduce()任务失败了，那依赖的失败的Map()任务必须重新运行。
  * Reduce worker宕机:
    - 已完成的任务不受影响 -- 保存在GFS里，并且有副本
    - master将还未完成的任务分配给别的worker执行
  * Reduce worker在输出结果的过程中宕机:
    - GFS的重命名具有原子性，在输出完成前对外不可见
    - 所以master将Reduce()任务分配给别的worker执行是安全的

### 其他失败/问题:
  * 如果master将同一个Map()任务分配给两个worker了怎么办？
    - 可能master错误地认为其中一个worker宕机了
    - master只会将其中一个worker产生的Map()结果传递给Reduce worker
  * 如果master将同一个Reduce()任务分配给两个worker了怎么办？
    - 两个worker都会尝试把相同的结果输出到GFS！
    - GFS重命名的原子性保证了两次输出结果不会混起来，只有一次结果最后对外可见
  * 如果某一个worker特别慢 -- 一个“拖后腿的”？
    - 可能是因为硬件有问题
    - master会将最后几个任务重新分发一次
  * 如果某一个worker由于硬件或者软件问题产生了错误的结果怎么办？
    - 那太糟糕了！MR默认worker的硬件和软件是正常工作的
  * 如果master宕机了怎么办？
    - 读取记录点恢复作业，或者放弃这次作业

### MapReduce不适用于什么样的应用场景？
  * 并不是所有的任务都适合 map/shuffle/reduce 的模式
  * 处理小量数据时，比如非网站后端应用，因为固定开销太大
  * 对大量数据的小量更新时，比如往一张大表里插入几条数据
  * 输入不可预期时 (Map和Reduce都不能自由选择输入)
  * 需要多次随机洗牌时，比如网页排序 (可以使用多个MR系统来解决这个问题，但是效率不高)
  * 更自由的系统需要使用更复杂的模型才能使用MR

### 现实中互联网公司是如何使用MapReduce的?
  * "CatBook"是一个做猫咪社交网络的公司，他们需要:
    - 1) 建立一个搜索索引，让人们可以找到其他人的猫
    - 2) 分析不同猫的受欢迎程度，以此来决定它们的广告价值
    - 3) 检测狗的存在并删除他们的账号
  * 可以用MapReduce来实现所有需求！
  * 每天晚上运行批量任务处理所有账号
    - 1) 建立倒排索引:

      ```
        map(profile text) -> (word, cat_id)
        reduce(word, list(cat_id) -> list(word, list(cat_id))
      ```

    - 2) 统计账号访问情况:

      ```
        map(web logs) -> (cat_id, "1")
        reduce(cat_id, list("1")) -> list(cat_id, count)
      ```

    - 3) 过滤账号:

      ```
        map(profile image) -> img analysis -> (cat_id, "dog!")
        reduce(cat_id, list("dog!")) -> list(cat_id)
      ```

### 结论
  * MapReduce以一己之力让大集群计算变得流行
  * MapReduce并不是最高效或者最灵活的分布式计算架构
  * MapReduce拥有很好的可扩展性
  * MapReduce是一个简单易用的框架 -- 失败和数据流转都被框架封装了
  * 这些都是现实中需要权衡的因素
  * 我们将在本课程中看到更加先进的后继者
  * 祝你们在实验课玩得开心！
